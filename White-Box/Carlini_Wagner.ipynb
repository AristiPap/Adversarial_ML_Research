{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Carlini_Wagner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUp0uQ22oCRirb3z+3VPd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AristiPap/Thesis_Stuff/blob/main/Carlini_Wagner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When adversarial examples were first discovered by Szegedy in 2013, the optimization problem to craft adversarial examples was formulated as:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1FIHFqZ2V4hwnd8cl7r-Pc54322PN2HZ5)\n",
        "\n",
        "where, x= input image, ùõø = perturbations, D = distance metric between the adversarial and real image, C = Classifier function, n= dimensions, t = target class. The distance metric is usually specified in terms of Lp norms (L0, L2 or L‚àû). Constraint 1 makes sure that the image is indeed misclassified and constraint 2 makes sure that the adversarial image is valid i.e. it lies within the normalized dimensions of x.\n",
        "\n",
        "Traditionally well known way to solve this optimization problem is to define an objective function (**generally but not necessarily a loss function that penalizes as we move further from satisfying the constraints**) and to perform gradient descent on it; which guides us to a optimal point in the function. However, The formulation above is difficult to solve because C ( x + ùõø ) = t is highly non-linear (the classifier is not a straight forward linear function). In the work by Szegedy in 2013, he resorts to solving the problem approximately, using 2nd order optimization technique known as L-BFGS.\n",
        "\n",
        "But In C&W attack, Carlini expresses constraint 1 in a different form as an objective function ‚Äòf‚Äô (the reasoning behind this is that this form is suited better for optimization) such that when C ( x + ùõø ) = t is satisfied f ( x + ùõø ) ‚â§ 0 is also satisfied\n",
        "\n",
        "Conceptually, the objective function tells us ‚Äúhow close we are getting to being classified as t‚Äù. One simple example fo the function ‚Äòf‚Äô, one that is not a very good choice but is well suited for explanation is:\n",
        "\n",
        "f = [1 - C(x+Œ¥)T]\n",
        "\n",
        "Where C( x + ùõø )T is the probability of x+ ùõø being classified as t. If the probability is low, value of f is closer to 1 whereas when it is classified as t, f is equal to 0"
      ],
      "metadata": {
        "id": "FXS88KAnzrJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the caveats of the C&W attack are:\n",
        "\n",
        "1) This attack algorithm can generate stronger/ higher quality adversarial examples but the cost to generate them is also higher (because of the way optimization problem is formulated). Even though, Carlini suggests some relaxations in the paper to reduce the cost, it is still costly.\n",
        "\n",
        "2) Compared to attack techniques like the Fast Gradient Sign Method (FGSM), another major difference here is that in the original formulation of the C&W attack problem, the authors do not specify a threshold that sets a limits the maximum distortion that is allowed. This is done in FGSM with a parameter ‚Äòellipson‚Äô. What this means is that, the attack here always succeeds.\n",
        "\n",
        "Carlini showed that adversarial examples are classified with a higher confidence than real images for C&W attack. This might be true for other attack algorithms as well."
      ],
      "metadata": {
        "id": "_Ge-SvWnyzKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ZAo8mxs-1klCKTLLdt8U7a7mvbxiErHJ)\n"
      ],
      "metadata": {
        "id": "hSQZuF1U2HN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib\n",
        "import datetime\n",
        "import time as t\n",
        "\n",
        "import matplotlib\n",
        "import tkinter\n",
        "from tkinter import ttk\n",
        "import threading\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import ImageTk, Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.vgg16 import decode_predictions, preprocess_input\n",
        "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg"
      ],
      "metadata": {
        "id": "M5eDodgskJZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN1sPdKh-u4M",
        "outputId": "f3bf3dbc-1a4b-4d6b-ff47-47d2da0efdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMX7YbEkaNpz",
        "outputId": "7c8967c7-c14d-4838-d842-77c415f67364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(model, perturbated_image, target):\n",
        "    \"\"\"\n",
        "    This method is used to calculate the loss function specified by carlini and wagner to be the most promising\n",
        "    to use for this attack.\n",
        "    See: https://ieeexplore.ieee.org/abstract/document/7958570 for more information\n",
        "    :param model: attacked model\n",
        "    :param perturbated_image: adversarial example\n",
        "    :param target: target class the perturbated_image should be misclassified as.\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "    # model(perturbated_image) returns [output, output_pre_activation] with output and output_pre_activation\n",
        "    # of shape [1 1000], so indexing by [1][0] returns output_pre_activation in shape [1000]\n",
        "    output_pre_activation = model(perturbated_image)[1][0]\n",
        "    max_z = tf.reduce_max(output_pre_activation)\n",
        "    z_t = output_pre_activation[target]\n",
        "    return tf.maximum(0.0, max_z - z_t)\n",
        "\n",
        "\n",
        "def load_and_format_image(path, size):\n",
        "    \"\"\"\n",
        "    This method is used to load the image stored at path, scale it to size and to pixel values in [0,1],\n",
        "    and converting it to a tf.Tensor with format NHWC.\n",
        "    :param path: The path the image to be loaded is stored.\n",
        "    :param size: The 2-dim. size of the image. An 224 by 224 rgb image has size (224,224) for example.\n",
        "    :return: formatted and rescaled image as NHWC tensor\n",
        "    \"\"\"\n",
        "    image = tf.keras.preprocessing.image.load_img(path)\n",
        "    image = image.resize(size)\n",
        "    preprocessed_img = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    # the carlini-wagner attack generates images with values in [0,1], so the loaded image with values in\n",
        "    # {0,1,..,255} needs to be rescaled\n",
        "    preprocessed_img = preprocessed_img / 255.0\n",
        "    preprocessed_img = tf.reshape(preprocessed_img, shape=(1,\n",
        "                                                           preprocessed_img.shape[0],\n",
        "                                                           preprocessed_img.shape[1],\n",
        "                                                           preprocessed_img.shape[2]))\n",
        "    return preprocessed_img\n",
        "\n",
        "\n",
        "def preprocess_model(model):\n",
        "    \"\"\"\n",
        "    The carlini-wagner attack relies on having an option to obtain the logits that serve as input to the\n",
        "    softmax layer, so the last layer of the model has to be split.\n",
        "    \"\"\"\n",
        "    model.layers[-1].activation = tf.identity\n",
        "    inputs = tf.keras.Input(shape=(224, 224, 3), name='input_layer')\n",
        "    # As the carlini wagner attack considers images to have pixel values in [0,1], but most image processing models\n",
        "    # use 8-bit color channels, we need to rescale by a factor of 255.\n",
        "    inputs_scaled = inputs * 255\n",
        "    output_pre_softmax = model(inputs_scaled)\n",
        "    #use softmax as this function was the best as said by carlini\n",
        "    output = tf.nn.softmax(output_pre_softmax)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=[output, output_pre_softmax])\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def perform_iteration(opt, model, input_img, target, w, c):\n",
        "    \"\"\"\n",
        "    This method implements a single update step using the optimizer opt, given the model, input image, target,\n",
        "    the learnable perturbation matrix w and the tradeoff variable c.\n",
        "    :w is learnable perturbation value\n",
        "    :c is tradeoff between distance and loss. High c means higher success rate, lower c means increased quality of\n",
        "              adversarial example\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(w)\n",
        "        delta = (1 / 2) * (tf.tanh(w) + 1) - input_img\n",
        "        loss_pre_softmax = loss(model, input_img + delta, target)\n",
        "        loss_distance = tf.norm(delta, ord='euclidean')\n",
        "        total_loss = loss_distance + c * tf.dtypes.cast(loss_pre_softmax, tf.dtypes.float32)\n",
        "    gradients = tape.gradient(total_loss, w)\n",
        "    opt.apply_gradients(zip([gradients], [w]))\n",
        "    return total_loss, loss_pre_softmax\n",
        "\n",
        "\n",
        "def to_img(input_image, w):\n",
        "    \"\"\"\n",
        "    This method is used to generate an image with pixel values in {0,1,..,255} represented as 4d np.ndarray.\n",
        "    \"\"\"\n",
        "    delta = (1 / 2) * (tf.tanh(w) + 1) - input_image\n",
        "    output_img = input_image + delta\n",
        "    image_shape = (-1, input_image.shape[1], input_image.shape[2], input_image.shape[3])\n",
        "    output_img_rescaled = output_img * 255\n",
        "    return np.uint8(np.reshape(output_img_rescaled.numpy(), image_shape))\n",
        "\n",
        "\n",
        "def summary_entry(file_writer, epoch, input_image, w, misclassification_loss, loss_val):\n",
        "    with file_writer.as_default():\n",
        "        if epoch % 100 == 0:\n",
        "            # to save disk space only every 100 steps a image is written to the summary\n",
        "            tf.summary.image(\"Perturbated Image\", to_img(input_image, w), step=epoch)\n",
        "        tf.summary.scalar(\"Misclassification Loss\", misclassification_loss, step=epoch)\n",
        "        tf.summary.scalar(\"Total Loss\", loss_val, step=epoch)\n",
        "\n",
        "\n",
        "def perform_attack(model, input_image, target, epochs=2500, c=1, tb=True, tb_path='./logs/cw'):\n",
        "    \"\"\"\n",
        "    carlini-wagner attack.Return: np.ndarray with pixel values in {0,1,...,255}\n",
        "    \"\"\"\n",
        "    # w is the parameter on which we perform learning steps, see to_img(input_image, w) to see how perturbation works.\n",
        "    w = tf.Variable(tf.zeros(tf.shape(input_image)), dtype=tf.dtypes.float32)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    file_writer = tf.summary.create_file_writer(tb_path)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        loss_val, misclassification_loss = perform_iteration(optimizer, model, input_image, target, w, c)\n",
        "        if tb:\n",
        "            summary_entry(file_writer, epoch, input_image, w, misclassification_loss, loss_val)\n",
        "\n",
        "    return to_img(input_image, w)[0]"
      ],
      "metadata": {
        "id": "rn_FrTl89gLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdversarialAttack:\n",
        "    \"\"\"\n",
        "    This class serves as a wrapper class for easier access to the cw attack\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.model = kwargs['model']\n",
        "        # set selected algorithm to one of  {fast_gradient_sign_method, carlini_wagner, one_pixel_attack}\n",
        "        attack_algorithm_name = kwargs['attack_algorithm']\n",
        "\n",
        "        \n",
        "        # the carlini wagner attack relies on using logits, so the softmax layer has to be split\n",
        "        self.model = preprocess_model(self.model)\n",
        "\n",
        "    def load_and_format_img(self, path, size):\n",
        "       \n",
        "        return load_and_format_image(path, size)\n",
        "\n",
        "    def perform_attack(self, input_image, *args, **kwargs):\n",
        "       \n",
        "        return perform_attack(self.model, input_image, *args, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_img_with_timestamp(image, directory, name=\"\"):\n",
        "        \"\"\"\n",
        "        This method can be used to save images with a timestamp included in the filename.\n",
        "        \"\"\"\n",
        "        timestamp = datetime.datetime.fromtimestamp(t.time()).strftime('%Y_%m_%d_%H_%M_%S')\n",
        "        # add an underscore if a name was given, to make it easier to read\n",
        "        name = name if name == '' else name + '_'\n",
        "        matplotlib.image.imsave('{}/{}{}.png'.format(directory, name, timestamp), image)"
      ],
      "metadata": {
        "id": "KiOcLD3okRkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labels_and_probabilities(model,image):\n",
        "        \"\"\"\n",
        "        Returns labels and self.net's corresponding estimated probabilities.\n",
        "        :param image: image for which probabilities should be estimated.\n",
        "        :return: list of labels and list of corresponding probabilities.\n",
        "        \"\"\"\n",
        "        predictions = [(label, probability) for _, label, probability in\n",
        "                       decode_predictions(model.predict(image), top=5)[0]]\n",
        "        labels = [a for a, _ in predictions]\n",
        "        probabilities = [b for _, b in predictions]\n",
        "        return labels, probabilities\n",
        "\n",
        "\n",
        "def display_barh(model, labels, probabilities):\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 4.5), dpi=75)\n",
        "\n",
        "        y_pos = np.arange(len(labels))\n",
        "\n",
        "        ax.barh(y_pos, probabilities, align='center', color='blue')\n",
        "        ax.set_yticks(y_pos)\n",
        "\n",
        "        ax.set_yticklabels(labels)\n",
        "        ax.invert_yaxis()  # labels read top-to-bottom\n",
        "        ax.set_xlabel('Probability\\n')\n",
        "        ax.set_title('Estimation of VGG16')\n",
        "        plt.show()\n",
        "\n",
        "def _load_and_format_image(path, size):\n",
        "        \"\"\"\n",
        "        This method is used to load the image stored at path, scale it to size and to pixel values in [0,1],\n",
        "        and converting it to a tf.Tensor with format NHWC.\n",
        "        :param path: The path the image to be loaded is stored.\n",
        "        :param size: The 2-dim. size of the image. An 224 by 224 rgb image has size (224,224) for example.\n",
        "        :return: formatted and rescaled image as NHWC tensor\n",
        "        \"\"\"\n",
        "        image = tf.keras.preprocessing.image.load_img(path)\n",
        "        image = image.resize(size)\n",
        "        preprocessed_img = tf.keras.preprocessing.image.img_to_array(image)\n",
        "        preprocessed_img = tf.reshape(preprocessed_img, shape=(1,\n",
        "                                                              preprocessed_img.shape[0],\n",
        "                                                              preprocessed_img.shape[1],\n",
        "                                                              preprocessed_img.shape[2]))\n",
        "        return preprocessed_img"
      ],
      "metadata": {
        "id": "7IIV_HtOdYno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.applications.VGG16()\n",
        "[(_, height, width, _)] = model.layers[0].input_shape\n",
        "size = height, width\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Thesis_notebooks/adversarials-master/files/images/sea_lion.jpg'\n",
        "aa = AdversarialAttack(attack_algorithm='carlini_wagner', model=model)\n",
        "input_image = aa.load_and_format_img(path, size)\n",
        "# target 291 meaning the target class is the lion, see: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "adv_img = aa.perform_attack(input_image, target=291, epochs=1000, tb_path='./logs/cw/sea_lion/')\n",
        "aa.save_img_with_timestamp(adv_img, directory='/content/drive/MyDrive/Thesis_notebooks/adversarials-master/files/images', name='cw_sea_lion_1000_epochs_t_291')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjHkdFngkXRS",
        "outputId": "3ed74bc5-8b32-4d9a-ea85-494a5b519c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [23:28<00:00,  1.41s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_path = '/content/drive/MyDrive/Thesis_notebooks/adversarials-master/files/images/cw_sea_lion_1000_epochs_t_291_2022_02_01_14_36_53.png'\n",
        "a_image = _load_and_format_image(a_path, size)\n",
        "labels, probabilities = labels_and_probabilities(model,a_image)\n",
        "print(labels)\n",
        "fig, ax = plt.subplots(figsize=(7, 4.5), dpi=75)\n",
        "\n",
        "y_pos = np.arange(len(labels))\n",
        "\n",
        "ax.barh(y_pos, probabilities, align='center', color='blue')\n",
        "ax.set_yticks(y_pos)\n",
        "\n",
        "ax.set_yticklabels(labels)\n",
        "ax.invert_yaxis()  # labels read top-to-bottom\n",
        "ax.set_xlabel('Probability\\n')\n",
        "ax.set_title('Estimation of VGG16')\n",
        "plt.savefig(\"mygraph.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_9_5zUJelSV",
        "outputId": "7b6550b7-53a2-49f1-9841-00216707c96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lion', 'warthog', 'hartebeest', 'impala', 'tusker']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_image = _load_and_format_image(path, size)\n",
        "labels, probabilities = labels_and_probabilities(model,b_image)\n",
        "print(labels)\n",
        "fig, ax = plt.subplots(figsize=(7, 4.5), dpi=75)\n",
        "\n",
        "y_pos = np.arange(len(labels))\n",
        "\n",
        "ax.barh(y_pos, probabilities, align='center', color='blue')\n",
        "ax.set_yticks(y_pos)\n",
        "\n",
        "ax.set_yticklabels(labels)\n",
        "ax.invert_yaxis()  # labels read top-to-bottom\n",
        "ax.set_xlabel('Probability\\n')\n",
        "ax.set_title('Estimation of VGG16')\n",
        "plt.savefig(\"mygraph_or.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXiS27kbvOAw",
        "outputId": "0b07e047-74e8-46ea-90c7-8efe6dac49e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sea_lion', 'platypus', 'hippopotamus', 'stingray', 'otter']\n"
          ]
        }
      ]
    }
  ]
}